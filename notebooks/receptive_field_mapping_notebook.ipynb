{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "# \ud83e\udde0 Receptive Field Mapping Notebook\n",
        "\n",
        "Welcome to the **Receptive Field Mapping Notebook** \u2014 a streamlined alternative to the Streamlit app for analyzing behaviorally-relevant neural activity through high-precision video tracking and spike data alignment.\n",
        "\n",
        "This notebook mirrors the Streamlit experience step by step while letting you stay inside Jupyter.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## \ud83d\udd0d What This Notebook Does\n",
        "\n",
        "- **Video Analysis with DeepLabCut** \u2014 Track motion of painted markers and colored filaments using your DeepLabCut project.\n",
        "- **Model Re-training** \u2014 Re-train the supplied model with your own labels when you need to adapt the detector to a new setup.\n",
        "- **Data Cleaning & Quality Control** \u2014 Detect outliers frame-to-frame and impute them with machine-learning models.\n",
        "- **Feature Extraction & Bending Detection** \u2014 Calculate bending coefficients from tracked filament coordinates over time.\n",
        "- **Spike-Time Alignment** \u2014 Synchronize neural recordings with behavioral events, including touch timestamps and filament bending.\n",
        "- **Interactive Visualization** \u2014 Plot synchronized motion and spike activity, create homography visualizations, and export receptive field videos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## \u2699\ufe0f Quick Background Summary\n",
        "\n",
        "This workflow uses **DeepLabCut** for marker tracking, aligns **neural spikes** to tracked behavioral events, and enables visualization directly in this notebook with **Plotly** and **Matplotlib** helpers from the project.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Backend logic flow for creating a labeled video then post-processing.](assets/flowchart.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## \u2705 Before You Start\n",
        "\n",
        "Make sure your recordings follow the setup instructions in the **Recording Instructions and Requirements** section below so that predictions and post-processing behave as expected.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## \ud83c\udfa5 Recording Instructions and Requirements\n",
        "\n",
        "For the recording to be properly recognized by the AI model, a few things need to be marked correctly to allow for accurate predictions and follow-up post-processing.\n",
        "\n",
        "### \ud83d\udccb Pre-filming Requirements\n",
        "\n",
        "The model is designed to detect:\n",
        "- 4 dots on the skin that represent the 4 corners of a square with sides of **1 or 2 cm**.\n",
        "- A filament with 3 **separate color zones**, allowing it to distinguish **6 points for bending**.\n",
        "\n",
        "#### \u2705 Lighting and Camera\n",
        "- Use **static, clean white lighting** for the subject.\n",
        "- Ensure a **stationary camera** throughout the recording. Either use a tripod or have steady arms.\n",
        "\n",
        "#### \u2705 Skin Dot Marking\n",
        "- Paint **4 dots** in the corners of a square using a **bright, opaque green** marker.\n",
        "  - Recommended: [Posca paint pens](https://www.posca.com/en/product/pc-5m/)\n",
        "- Avoid having other objects or markings in the frame that could be confused with the dots.\n",
        "- Example of bright green dots on skin, in a well-lit setting:\n",
        "\n",
        "![Example: Bright green dots on skin](assets/dots_example.png)\n",
        "\n",
        "#### \u2705 Filament Marking\n",
        "- Paint the filament with **2 distinct opaque colors in pattern**.\n",
        "- This allows the model to detect **6 separate points** for bend analysis.\n",
        "- Avoid similar colors or objects to the filament that could confuse the model.\n",
        "- Example of a filament painted white and dark blue:\n",
        "\n",
        "![Example: Colored filament with clear zones](assets/filament_example.png)\n",
        "\n",
        "#### \u2705 Clean the Skin\n",
        "- Ensure no old marks or blemishes interfere with detection.\n",
        "\n",
        "### \ud83c\udfac During-Filming Requirements\n",
        "- Start the video with **5 touches** at the hotspot \u2014 one per second \u2014 for synchronization with neuron data.\n",
        "- Ensure **no other filaments are visible** in the frame.\n",
        "- Confirm that the **filament is clearly visible and not blurry** during bending.\n",
        "\n",
        "<table>\n",
        "  <tr>\n",
        "    <td><img src=\"assets/bad_bend_example_1.png\" width=\"220\"><br>\u274c Blurry region of interest</td>\n",
        "    <td><img src=\"assets/bad_bend_example_2.png\" width=\"220\"><br>\u274c Poor bend angle</td>\n",
        "    <td><img src=\"assets/good_bend_example.png\" width=\"220\"><br>\u2705 Clear, visible bend</td>\n",
        "  </tr>\n",
        "</table>\n",
        "\n",
        "> \u2705 Double-check recordings to ensure clear visibility of the bend and proper lighting.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## \ud83c\udfac Step 1 \u2014 DeepLabCut Video Prediction\n",
        "\n",
        "This section mirrors the Streamlit **Create Labeled Video** and **Labeling / Retraining** tabs. Use it to initialise your DeepLabCut project, preprocess a video, generate predictions, extract frames for labeling, and kick off retraining when needed.\n",
        "\n",
        "Run the setup cell below first, then interact with the widgets to walk through the workflow.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "from IPython.display import display\n",
        "\n",
        "from src.notebook_ui import attach_streamlit_shim, build_prediction_tabs, build_post_processing_tabs\n",
        "\n",
        "NOTEBOOK_STATE, NOTEBOOK_SHIM = attach_streamlit_shim()\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "display(build_prediction_tabs(NOTEBOOK_STATE, NOTEBOOK_SHIM))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### \ud83d\udcdd Napari Labeling Checklist\n",
        "\n",
        "Follow these instructions while labeling in Napari after you click **Extract frames & launch Napari** above:\n",
        "\n",
        "1. Open `Plugins \u2192 Keypoint controls` in Napari (dismiss the tutorial pop-up if it appears).\n",
        "2. Choose `File \u2192 Open File(s)\u2026`, navigate to the DeepLabCut project folder, and open `config.yaml`.\n",
        "3. Choose `File \u2192 Open Folder\u2026`, navigate into the new folder inside `labeled-data` that matches your video name, and select it. Then pick the `napari DeepLabCut` layer to start labeling.\n",
        "4. After labeling at least five frames, save with `File \u2192 Save Selected Layer(s)` while `CollectedData` is selected, then close Napari before retraining.\n",
        "\n",
        "Refer to the screenshots in the `assets/` folder (same images used in the Streamlit app) if you need a visual reminder.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## \ud83d\udcca Step 2 \u2014 Post Processing\n",
        "\n",
        "Use the interface below to reproduce the Streamlit **Post Processing** page. It is organised into three tabs:\n",
        "\n",
        "1. **Labeled Data** \u2014 upload DLC prediction output, clean outliers, compute bending coefficients, and apply homography.\n",
        "2. **Neuron Data** \u2014 upload neural recordings, inspect them, and downsample to match the video rate.\n",
        "3. **Merged Data** \u2014 align both data sources, visualise receptive field maps, and export animations.\n",
        "\n",
        "Run the cell below to load the widgets, then work through the tabs from left to right.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "display(build_post_processing_tabs(NOTEBOOK_STATE, NOTEBOOK_SHIM))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## \u2705 Next Steps\n",
        "\n",
        "- Step through the tabs from left to right to mirror the Streamlit workflow.\n",
        "- Re-run individual widget sections whenever you adjust parameters; results will update live.\n",
        "- Generated videos can be saved by right-clicking in the output or by adapting the helper functions to write to disk.\n",
        "- Once satisfied, proceed with your downstream analysis or export the processed dataframes for further work.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}